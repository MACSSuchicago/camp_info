---
title: Maximum likelihood estimation and linear regression
---

```{r common-r, code = xfun::read_utf8(here::here("_common.R")), include = FALSE}
```

```{r setup}
library(tidyverse)
library(patchwork)
```

# Derive the maximum likelihood estimator

Alice models the time that she spends each week on homework as an exponentially distributed random variable with unknown parameter $\theta$. Homework times in different weeks are independent. After spending $10, 14, 18, 8, 20$ hours in the first five weeks of the quarter, what is her maximum likelihood estimate of $\theta$? **Be sure to show your work.**^[BT 9.1]

<!--

**Solution:** Let $X_i$ denote the random homework time for the $i$th week, $i = 1, \ldots, 5$. We have the observation vector $X = x$, where $x = (10, 14, 18, 8, 20)$. In view of the independence of the $X_i$, for $\theta \in [0, 1]$, the likelihood function is

$$
\begin{aligned}
f_X(x; \theta) &= f_{X_1} (x_1; \theta) \ldots f_{X_5}(x_5 ; \theta) \\
&= \theta e^{-x_1 \theta} \times \ldots \times \theta e^{-x_5 \theta} \\
&= \theta^5 e^{-(x_1 + \ldots + x_5) \theta} \\
&= \theta^5 e^{-(10 + 14 + 18 + 8 + 20) \theta} \\
&= \theta^5 e^{-71 \theta}
\end{aligned}
$$

To derive the ML estimate, we set to 0 the derivative of $f_X (x; \theta)$ with respect to $\theta$, obtaining (via the product rule)

$$
\begin{aligned}
\frac{\partial}{\partial \theta} (\theta^5 e^{-71 \theta}) &= 5 \theta^4 e^{-71 \theta} - 71 \theta^5 e^{-71\theta} \\
&= (5 - 71 \theta) \theta^4 e^{-71 \theta}
\end{aligned}
$$

We could set this equation equal to $0$ and solve for the roots of the equation. However intuition provides a helpful guide here. If $\hat{theta} = 0$, $\theta^4$ causes the function to simplify to $0$. Likewise, look at the first term $(5 - 71 \theta)$. If $\theta = \frac{5}{71}$, this term also reduces to 0. By the definition of the exponential distribution, the rate parameter $\theta$ cannot be 0. So the only valid solution here is

$$
\begin{aligned}
\hat{\theta} &= \frac{5}{71} \\
&= \frac{5}{x_1 + \ldots + x_5}
\end{aligned}
$$

-->

# Identify relationships

For each of the six plots, identify the strength of the relationship (e.g., weak, moderate, or strong) in the data and whether fitting a linear model would be reasonable.^[OpenIntro Modern Stats 7.4]

```{r}
library(openintro)
library(tidyverse)

set.seed(9271)

df <- tibble(
  x              = seq(-3, 4, 0.05),
  s              = -0.5 * x^3 + x^2 + x + rnorm(length(x), mean = 0, sd = 2),
  hockey_stick   = 2 * x^4 + -0.5 * x^3 + x^2 + x + rnorm(length(x), mean = 0, sd = 30),
  pos_lin_strong = 3 * x + rnorm(length(x), mean = 0, sd = 2),
  pos_weak       = 3 * x + rnorm(length(x), mean = 0, sd = 20),
  pos_weaker     = -3 * x + rnorm(length(x), mean = 0, sd = 10),
  neg_lin_weak   = -3 * x + rnorm(length(x), mean = 0, sd = 5) 
  ) %>%
  pivot_longer(cols = -x, names_to = "type", values_to = "y") %>%
  arrange(type) %>%
  mutate(type_label = case_when(
      type == "s"              ~ "(a)",
      type == "hockey_stick"   ~ "(b)",
      type == "pos_lin_strong" ~ "(c)",
      type == "pos_weak"       ~ "(d)",
      type == "pos_weaker"     ~ "(e)",
      type == "neg_lin_weak"   ~ "(f)",
  ))
    
ggplot(df, aes(x = x, y = y)) +
  geom_point(size = 2, alpha = 0.8) +
  facet_wrap(~type_label, scales = "free_y", nrow = 2) +
  theme(axis.text = element_blank()) +
  labs(x = NULL, y = NULL)
```
    
<!--

**Solution**:

a. Strong relationship, but a straight line would not fit the data.
a. Strong relationship, but a straight line would not fit the data.
a. Strong relationship, and a linear fit would be reasonable.
a. Weak relationship, and trying a linear fit would be reasonable.
a. Weak relationship, and trying a linear fit would be reasonable.
a. Moderate relationship, and a linear fit would be reasonable.

-->

# Starbucks, calories, and carbs

The scatterplot below shows the relationship between the number of calories and amount of carbohydrates (in grams) Starbucks food menu items contain. Since Starbucks only lists the number of calories on the display items, we might be interested in predicting the amount of carbs a menu item has based on its calorie content.^[OpenIntro Modern Stats 7.20]

```{r fig.asp=0.5}
library(openintro)
library(ggplot2)
library(broom)
library(patchwork)

p_1 <- ggplot(starbucks, aes(y = carb, x = calories)) +
  geom_smooth(method = "lm", color = "darkgray", se = FALSE) +
  geom_point(size = 2) +
  labs(
    x = "Calories",
    y = "Carbs (grams)"
    )

m <- lm(carb ~ calories, data = starbucks)
m_aug <- augment(m)

p_2 <- ggplot(m_aug, aes(x = .fitted, y = .resid)) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_point(size = 2) +
  labs(
    x = "Predicted carbs (grams)",
    y = "Residuals"
    )

p_1 + p_2
```

a. Describe the relationship between number of calories and amount of carbohydrates (in grams) that Starbucks food menu items contain.
    
    <!--
    
    **Solution**: There is a positive, moderate, linear association between number of calories and amount of carbohydrates. In addition, the amount of carbohydrates is more variable for menu items with higher calories, indicating non-constant variance. There also appear to be two clusters of data: a patch of about a dozen observations in the lower left and a larger patch on the right side.
    
    -->

a. In this scenario, what are the predictor and outcome variables?

    <!--
    
    **Solution**: Predictor: number of calories. Outcome: amount of carbohydrates (in grams).
    
    -->

a. Why might we want to fit a regression line to these data?

    <!--
    
    **Solution**: With a regression line, we can predict the amount of carbohydrates for a given number of calories. This may be useful if only calorie counts for the food items are posted but the amount of carbohydrates in each food item is not readily available.
    
    -->

a. What does the residuals vs. predicted plot tell us about the variability in our prediction errors based on this model for items with lower vs. higher predicted carbs?

    <!--
    
    **Solution**: Food menu items with higher predicted carbs are predicted with higher variability than those without, suggesting that the model is doing a better job predicting carbs amount for food menu items with lower predicted proteins.
    
    -->

# Identifying outliers

Identify the outliers in the scatterplots shown below and determine what type of outliers they are. Explain your reasoning.^[OpenIntro Modern Stats 7.26]

```{r fig.asp=0.33, out.width="100%"}
library(openintro)
library(tidyverse)
library(patchwork)

set.seed(83629)

df <- tibble(
  x = seq(1,50,1),
  y = 3 * x + 3 + rnorm(length(x), mean = 0, sd = 10),
  x_influential = c(x[1:49], -50),
  y_influential = c(y[1:49], -300),
  y_outlier = c(y[1:25], y[26]+100, y[27:50])
)

p_1 <- ggplot(df, aes(x = x_influential, y = y)) +
  geom_smooth(method = "lm", color = "darkgray", se = FALSE) +
  geom_point() +
  labs(x = NULL, y = NULL) +
  theme(axis.text = element_blank())

p_2 <- ggplot(df, aes(x = x_influential, y = y_influential)) +
  geom_smooth(method = "lm", color = "darkgray", se = FALSE) +
  geom_point() +
  labs(x = NULL, y = NULL) +
  theme(axis.text = element_blank())

p_3 <- ggplot(df, aes(x = x, y = y_outlier)) +
  geom_smooth(method = "lm", color = "darkgray", se = FALSE) +
  geom_point() +
  labs(x = NULL, y = NULL) +
  theme(axis.text = element_blank())

p_1 + p_2 + p_3
```

<!--

**Solution**:

a. The outlier is in the upper-left corner. Since it is horizontally far from the center of the data, it is an influential point. Additionally, since the fit of the regression line is greatly influenced by this point, it is a point with high leverage.
a. The outlier is located in the lower-left corner. It is horizontally far from the rest of the data, so it is a high-leverage point. The regression line also would fall relatively far from this point if the fit excluded this point, meaning it the outlier is influential.
a. The outlier is in the upper-middle of the plot. Since it is near the horizontal center of the data, it is not a high-leverage point. This means it also will have little or no influence on the slope of the regression line.
-->

# High correlation, good or bad?

Two friends, Frances and Annika, are in disagreement about whether high correlation values are *always* good in the context of regression. Frances claims that it's desirable for all variables in the dataset to be highly correlated to each other when building linear models. Annika claims that while it's desirable for each of the predictors to be highly correlated with the outcome, it is not desirable for the predictors to be highly correlated with each other.

Who is right: Frances, Annika, both, or neither? Explain your reasoning using appropriate terminology.^[OI Modern Stats 8.1]

<!--

**Solution**: Annika is right. All variables being highly correlated, including the predictor variables being highly correlated with each other, is not desirable as this would result in multicollinearity.

-->

# Training for the 5K

Nico signs up for a 5K (a 5,000 metre running race) 30 days prior to the race. They decide to run a 5K every day to train for it, and each day they record the following information: `days_since_start` (number of days since starting training), `days_till_race` (number of days left until the race), `mood` (poor, good, awesome), `tiredness` (1-not tired to 10-very tired), and `time` (time it takes to run 5K, recorded as mm:ss). Top few rows of the data they collect is shown below.

```{r}
library(tidyverse)
tribble(
  ~days_since_start, ~days_till_race, ~mood, ~tiredness, ~time,
  "1", "29", "good", "3", "25:45",
  "2", "28", "poor", "5", "27:13",
  "3", "27", "awesome", "4", "24:13",
  "...", "...", "...", "...", "..."
) %>%
  kbl()
```

Using these data Nico wants to build a model predicting `time` from the other variables. Should they include all variables shown above in their model? Why or why not?^[OI Modern Stats 8.3]

<!--

**Solution**: No, they shouldn’t include all variables as `days_since_start` and `days_since_race` are perfectly correlated with each other. They should only include one of them.

-->

# Multiple regression fact checking

Determine which of the following statements are true and false. For each statement that is false, explain why it is false.^[OI Modern Stats 8.4]

a. If predictors are collinear, then removing one variable will have no influence on the point estimate of another variable's coefficient.

    <!--
    
    **Solution**: False. When predictors are collinear, it means they are correlated, and the inclusion of one variable can have a substantial influence on the point estimate (and standard error) of another.
    
    -->

a. Suppose a numerical variable $x$ has a coefficient of $b_1 = 2.5$ in the multiple regression model. Suppose also that the first observation has $x_1 = 7.2$, the second observation has a value of $x_1 = 8.2$, and these two observations have the same values for all other predictors. Then the predicted value of the second observation will be 2.5 higher than the prediction of the first observation based on the multiple regression model.

    <!--
    
    **Solution**: True.
    
    -->

a. If a regression model's first variable has a coefficient of $b_1 = 5.7$, then if we are able to influence the data so that an observation will have its $x_1$ be 1 larger than it would otherwise, the value $y_1$ for this observation would increase by 5.7.

    <!--
    
    **Solution**: False. This would only be the case if the data was from an experiment and $x_1$ was one of the variables set by the researchers. (Multiple regression can be useful for forming hypotheses about causal relationships, but it offers zero guarantees.)
    
    -->

# Movie returns by genre

A model was fit to predict return-on-investment (ROI) on movies based on release year and genre (Adventure, Action, Drama, Horror, and Comedy). The plots below show the predicted ROI vs. actual ROI for each of the genres separately. Do these figures support the comment in the FiveThirtyEight.com article that states, "The return-on-investment potential for horror movies is absurd." Note that the $x$-axis range varies for each plot.^[OI Modern Stats 8.8]

```{r}
library(lubridate)
movie_profit <- read_csv(here::here("data", "movie_profit.csv"))
movie_profit <- movie_profit %>%
  mutate(
    release_date = mdy(release_date),
    release_year = year(release_date),
    oct_release = ifelse(month(release_date) == 10, "yes", "no"),
    dom_gross_to_prod = domestic_gross / production_budget,
    ww_gross_to_prod = worldwide_gross / production_budget
    ) %>%
  filter(
    release_year >= 2010,
    release_year < 2019
    )

m_movie <- lm(ww_gross_to_prod ~ release_year + genre, data = movie_profit)
m_movie_aug <- augment(m_movie)

ggplot(m_movie_aug, aes(y = .fitted, x = ww_gross_to_prod, color = genre)) + 
  geom_point(alpha = 0.5, show.legend = FALSE, size = 2) +
  facet_wrap(~genre, scales = "free_x") +
  theme_minimal() + 
  labs(x = "Actual ROI", y = "Predicted ROI", color = "Genre") +
  scale_color_openintro("five")
````

<!--

**Solution**: While the model is not doing a good fit for any genre, it is under-predicting return-on-investment for horror movies a lot more than other genres. This is in line with the FiveThirtyEight article, since it suggests the margins are unusually high for horror movies.

-->

# Murders and poverty

The table below shows the output of a linear model annual murders per million (`annual_murders_per_mil`) from percentage living in poverty (`perc_pov`) in a random sample of 20 metropolitan areas.^[OI Modern Stats 24.6]

```{r}
library(MASS)
library(broom)

lm(annual_murders_per_mil ~ perc_pov, data = murders) %>%
  tidy() %>%
  mutate(p.value = ifelse(p.value < 0.0001, "<0.0001", round(p.value, 4))) %>%
  kbl(digits = 4)
```

a. What are the hypotheses for evaluating whether the slope of the model predicting annual murder rate from poverty percentage is different than 0?

    <!--
    
    **Solution**: $H_0: \beta_1 = 0; H_A: \beta_1 \neq 0$
    
    -->

a. State the conclusion of the hypothesis test from part (a) in context of the data. What does this say about whether poverty percentage is a useful predictor of annual murder rate?

    <!--
    
    **Solution**: The p-value for this test is approximately 0, therefore we reject $H_0$. The data provide convincing evidence that the slope of the model predicting annual murder rate from poverty percentage is different than 0. This implies that poverty percentage is a useful predictor of murder rate.
    
    -->

a. Calculate a 95% confidence interval for the slope of poverty percentage, and interpret it in context of the data.

    <!--
    
    **Solution**: $n=20$, $df=18$, $T^{*}_{18}=2.10$, $2.559 \pm 2.10 \times 0.390 = (1.74,3.378)$. For each percentage point poverty is higher, murder rate is expected to be higher on average by 1.74 to 3.378 per million.
    
    -->

a. Do your results from the hypothesis test and the confidence interval agree? Explain.

    <!--
    
    **Solution**: Yes, we rejected $H_0$ and the confidence interval does not include 0.
    
    -->

# GPA

In this exercise we work with data from a survey of 55 Duke University students who were asked about their GPA, number of hours they sleep nightly, and number of nights they go `out` each week.

The plots below describe the show the distribution of each of these variables (on the diagonal) as well as provide information on the pairwise correlations between them.

Also provided below are three regression model outputs: `gpa` vs. `out`, `gpa` vs. `sleepnight`, and `gpa` vs. `out + sleepnight`.^[OI Modern Stats 25.2]

```{r}
library(tidyverse)
library(openintro)
library(broom)
library(GGally)

lm(gpa ~ out, data = gpa) %>%
  tidy() %>%
  mutate(p.value = ifelse(p.value < 0.0001, "<0.0001", round(p.value, 4))) %>%
  kbl(digits = 3)

lm(gpa ~ sleepnight, data = gpa) %>%
  tidy() %>%
  mutate(p.value = ifelse(p.value < 0.0001, "<0.0001", round(p.value, 4))) %>%
  kbl(digits = 3)

lm(gpa ~ out + sleepnight, data = gpa) %>%
  tidy() %>%
  mutate(p.value = ifelse(p.value < 0.0001, "<0.0001", round(p.value, 4))) %>%
  kbl(digits = 3)
```

```{r out.width = "100%", fig.asp = 1}
gpa %>%
  dplyr::select(sleepnight, out, gpa) %>%
  ggpairs(diag = list(continuous = "barDiag"),
          switch = "both") +
  theme(legend.position = "none",
    panel.grid.major = element_blank(),
    axis.ticks = element_blank(),
    axis.title.x = element_text(angle = 180, vjust = 1, color = "black"),
    panel.border = element_rect(fill = NA))
```

a. There are three variables described in the figure, and each is paired with each other to create three different scatterplots. Rate the pairwise relationships from most correlated to least correlated.

    <!--
    
    **Solution**: Highest correlation is `out` and `sleepnight` ($r = 0.382$); next is `gpa` and `out` ($r = 0.136$); least correlated is `gpa` and `sleepnight` ($r = 0.061$).

    -->

a. When using only one variable to model `gpa`, is `out` a significant predictor variable?  Is `sleepnight` a significant predictor variable?  Explain.

    <!--
    
    **Solution**: In the single variable linear regressions, neither `out` nor `sleepnight` are significant predictors of `gpa` as indicated by the high p-values on each of the coefficients in the separate models.
    
    -->

a. When using both `out` and `sleepnight` to predict `gpa` in a multiple regression model, are either of the variables significant? Explain.

    <!--
    
    **Solution**: When both `out` and `sleepnight` are used in the multiple linear regression model, neither of the variables become significant predictors of `gpa`. Indeed, when variables are insignificant in the single variable regression, it is unusual for them to be significant in a multiple regression model (it only happens when the variables have very specific relationships). Notice from the scatterplots that neither `out` nor `sleepnight` is very highly correlated with `gpa`.
    
    -->

# Movie returns

A FiveThirtyEight.com article reports that "Horror movies get nowhere near as much draw at the box office as the big-time summer blockbusters or action/adventure movies, but there's a huge incentive for studios to continue pushing them out. The return-on-investment potential for horror movies is absurd." To investigate how the return-on-investment (ROI) compares between genres and how this relationship has changed over time, an introductory statistics student fit a linear regression model to predict the ratio of gross revenue of movies to the production costs from genre and release year for 1,070 movies released between 2000 and 2018. Using the plots given below, determine if this regression model is appropriate for these data. In particular, use the residual plot to check the **LINE conditions** (linearity, independence of observations, normality, and constant or equal variability).^[OI Modern Stats 25.5]

```{r out.width = "100%", fig.asp = 1, fig.width = 8}
library(tidyverse)
library(lubridate)
library(broom)

movie_profit <- read_csv("data/movie_profit.csv")
movie_profit <- movie_profit %>%
  mutate(
    release_date = mdy(release_date),
    release_year = year(release_date),
    oct_release = ifelse(month(release_date) == 10, "yes", "no"),
    dom_gross_to_prod = domestic_gross / production_budget,
    ww_gross_to_prod = worldwide_gross / production_budget
    ) %>%
  filter(
    release_year >= 2010,
    release_year < 2019
    )

m_movie <- lm(ww_gross_to_prod ~ release_year + genre, data = movie_profit)
m_movie_aug <- augment(m_movie) %>%
    rownames_to_column()

p_res_hist <- ggplot(m_movie_aug, aes(x = .resid)) +
  geom_histogram() +
  labs(x = "Residual", y = "Count", title = "Histogram of residuals")

p_res_order <- ggplot(m_movie_aug, aes(y = .resid, x = as.numeric(rowname))) +
  geom_point() +
  labs(x = "Data order", y = "Residual", title = "Residuals vs. data order")

p_res_pred <- ggplot(m_movie_aug, aes(x = .fitted, y = .resid, color = genre)) +
  geom_point(alpha = 0.8, size = 1) +
  labs(x = "Predicted ROI", y = "Residual", color = "Genre", title = "Residuals vs. predicted values") +
  scale_color_openintro("five") +
  theme(legend.position = "bottom")

(p_res_hist + p_res_order) /
    p_res_pred
```

<!--

**Solution**:

- Linearity: Horror movies seem to show a much different pattern than the other genres. While the residuals plots show a random scatter over years and in order of data collection, there is a clear pattern in residuals for various genres, which signals that this regression model is not appropriate for these data.
- Independent observations: The variability of the residuals is higher for data that comes later in the dataset. We don’t know if the data are sorted by year, but if so, there may be a temporal pattern in the data that voilates the independence condition.
- Normality: The residuals are right skewed (skewed to the high end).
- Constant or Equal variability: The residuals vs. predicted values plot reveals some outliers. This plot for only babies with predicted birth weights between 6 and 8.5 pounds looks a lot better, suggesting that for bulk of the data the constant variance condition is met.

-->




